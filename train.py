"""
================================================================================
ĞĞ²Ñ‚Ğ¾Ñ€: Ğ‘Ğ°Ğ´Ñ€Ñ…Ğ°Ğ½Ğ¾Ğ² ĞÑĞ»Ğ°Ğ½-Ğ±ĞµĞº ĞŸĞ¾Ğ»Ğ°Ğ´Ğ¾Ğ²Ğ¸Ñ‡
Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ: ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¼Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ² ĞĞ½Ğ´Ñ€ĞµĞ¹ Ğ¡ĞµÑ€Ğ³ĞµĞµĞ²Ğ¸Ñ‡
Ğ¢ĞµĞ¼Ğ° Ğ’ĞšĞ : "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµĞ±ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ"
ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Occupancy Network Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ PIX3D
Ğ”Ğ°Ñ‚Ğ°: 2026
================================================================================

ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Occupancy Network:

    1. Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ”ĞĞĞĞ«Ğ¥
       - Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ±ĞµĞ»Ğ¸ [B, 3, 224, 224]
       - 3D Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ [B, N, 3]
       - Ground truth occupancy [B, N] (0=ÑĞ½Ğ°Ñ€ÑƒĞ¶Ğ¸, 1=Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸)

    2. FORWARD PASS
       - Encoder: Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ â†’ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ [B, 512]
       - PositionalEncoding: Ñ‚Ğ¾Ñ‡ĞºĞ¸ [B, N, 3] â†’ [B, N, 63]
       - Decoder: [latent, points_enc] â†’ logits [B, N]

    3. LOSS COMPUTATION
       - BCE Loss: Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸
       - IoU Loss: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ IoU
       - Total = BCE + 0.5 * IoU

    4. BACKWARD PASS
       - Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
       - Gradient clipping
       - ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· AdamW

Ğ—Ğ°Ğ¿ÑƒÑĞº:
    python train.py
    
    # Ğ˜Ğ»Ğ¸ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ (Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ):
    python train.py --batch_size 64 --num_epochs 300 --category chair
"""

import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR

import os
import sys
import signal
import argparse
from datetime import datetime
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ Ğ¸Ğ· Ğ½Ğ°ÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹
from config import get_config, update_config
from model import create_model
from datasets import Pix3DDataset, PreprocessedPix3DDataset, collate_fn, create_dataset
from loss import OccupancyLoss


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRACEFUL SHUTDOWN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

STOP_TRAINING = False


def signal_handler(signum, frame):
    """ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ graceful shutdown."""
    global STOP_TRAINING
    print("\n" + "=" * 60)
    print("[train.py] ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ (Ctrl+C Ğ¸Ğ»Ğ¸ SIGTERM)")
    print("[train.py] Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞ°Ñ Ñ‚ĞµĞºÑƒÑ‰ÑƒÑ ÑĞ¿Ğ¾Ñ…Ñƒ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚...")
    print("=" * 60)
    STOP_TRAINING = True


# Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞ›ĞĞ¡Ğ¡ TRAINER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Trainer:
    """
    ĞšĞ»Ğ°ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Occupancy Network.
    
    Ğ˜Ğ½ĞºĞ°Ğ¿ÑÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚:
        - ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€
        - Ğ¦Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸
        - Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ/Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ²
        - Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
    
    Args:
        cfg: ĞĞ±ÑŠĞµĞºÑ‚ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ (Ğ¸Ğ· config.py)
    """
    
    def __init__(self, cfg):
        self.cfg = cfg
        self.device = cfg.device
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        print(f"[train.py] Device: {self.device}")
        
        if self.device == 'cuda':
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"[train.py] GPU: {gpu_name} ({gpu_memory:.1f} GB)")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        self.model = create_model(
            latent_dim=cfg.model.latent_dim,
            num_frequencies=cfg.model.num_frequencies
        ).to(self.device)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Loss Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        self.criterion = OccupancyLoss(
            bce_weight=1.0,
            iou_weight=0.5
        )
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° AdamW
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=cfg.train.learning_rate,
            weight_decay=cfg.train.weight_decay,
            betas=(0.9, 0.999)
        )
        
        print(f"[train.py] Learning rate: {cfg.train.learning_rate}")
        print(f"[train.py] Batch size: {cfg.train.batch_size}")
        print(f"[train.py] Epochs: {cfg.train.num_epochs}")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Learning Rate Scheduler
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        # Warmup scheduler
        warmup_scheduler = LinearLR(
            self.optimizer,
            start_factor=0.01,
            end_factor=1.0,
            total_iters=cfg.train.warmup_epochs
        )
        
        # Cosine scheduler
        cosine_scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=max(cfg.train.num_epochs - cfg.train.warmup_epochs, 1),
            eta_min=1e-6
        )
        
        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼
        self.scheduler = SequentialLR(
            self.optimizer,
            schedulers=[warmup_scheduler, cosine_scheduler],
            milestones=[cfg.train.warmup_epochs]
        )
        
        print(f"[train.py] Scheduler: Warmup({cfg.train.warmup_epochs}) + Cosine")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Automatic Mixed Precision (AMP)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        self.use_amp = cfg.use_amp
        if self.use_amp:
            self.scaler = torch.amp.GradScaler('cuda')
            print("[train.py] AMP (FP16) enabled")
        else:
            self.scaler = None
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Tracking Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        self.best_iou = 0.0
        self.start_epoch = 0
        self.current_epoch = 0
    
    def save_checkpoint(
        self,
        epoch: int,
        is_best: bool = False,
        is_periodic: bool = False,
        reason: str = ""
    ) -> None:
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."""
        
        os.makedirs(self.cfg.paths.checkpoint_dir, exist_ok=True)
        
        state = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None,
            'epoch': epoch,
            'best_iou': self.best_iou,
            'config': {
                'latent_dim': self.cfg.model.latent_dim,
                'num_frequencies': self.cfg.model.num_frequencies,
                'type': 'global'
            },
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
            'train_config': {
                'batch_size': self.cfg.train.batch_size,
                'learning_rate': self.cfg.train.learning_rate,
                'num_epochs': self.cfg.train.num_epochs,
                'num_points': self.cfg.train.num_points,
                'category_filter': self.cfg.train.category_filter,
            }
        }
        
        # Ğ’ÑĞµĞ³Ğ´Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ latest
        latest_path = os.path.join(self.cfg.paths.checkpoint_dir, 'latest.pth')
        torch.save(state, latest_path)
        
        if reason:
            print(f"[train.py] ğŸ’¾ Checkpoint saved: {reason}")
        
        if is_best:
            best_path = os.path.join(self.cfg.paths.checkpoint_dir, 'model.pth')
            torch.save(state, best_path)
            print(f"[train.py] â­ Best model saved (IoU: {self.best_iou:.4f})")
        
        if is_periodic:
            periodic_path = os.path.join(
                self.cfg.paths.checkpoint_dir,
                f'epoch_{epoch + 1:03d}.pth'
            )
            torch.save(state, periodic_path)
            print(f"[train.py] ğŸ’¾ Periodic checkpoint: epoch_{epoch + 1:03d}.pth")
    
    def load_checkpoint(self, path: str) -> bool:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."""
        
        if not os.path.exists(path):
            print(f"[train.py] Checkpoint not found: {path}")
            return False
        
        print(f"[train.py] Loading checkpoint: {path}")
        
        try:
            checkpoint = torch.load(
                path,
                map_location=self.device,
                weights_only=False
            )
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            if 'scheduler_state_dict' in checkpoint:
                try:
                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                except Exception:
                    print("[train.py] Warning: scheduler state incompatible, resetting")
            
            if self.scaler and checkpoint.get('scaler_state_dict'):
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
            
            self.start_epoch = checkpoint.get('epoch', 0) + 1
            self.best_iou = checkpoint.get('best_iou', 0.0)
            
            print(f"[train.py] Resuming from epoch {self.start_epoch}")
            print(f"[train.py] Best IoU so far: {self.best_iou:.4f}")
            
            return True
            
        except Exception as e:
            print(f"[train.py] Error loading checkpoint: {e}")
            return False
    
    def train_epoch(self, loader: DataLoader, epoch: int) -> tuple:
        """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾Ñ…Ğ¸."""
        global STOP_TRAINING
        
        self.model.train()
        
        total_loss = 0.0
        total_iou = 0.0
        n_batches = 0
        
        pbar = tqdm(
            loader,
            desc=f"Epoch {epoch + 1}/{self.cfg.train.num_epochs}",
            ncols=100,
            leave=False
        )
        
        for batch in pbar:
            if STOP_TRAINING:
                print("\n[train.py] Stopping training loop...")
                break
            
            if batch is None:
                continue
            
            images = batch['image'].to(self.device, non_blocking=True)
            points = batch['points'].to(self.device, non_blocking=True)
            targets = batch['occupancies'].to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad(set_to_none=True)
            
            with torch.amp.autocast('cuda', enabled=self.use_amp):
                logits = self.model(images, points)
                loss_dict = self.criterion(logits, targets)
                loss = loss_dict['total']
            
            if self.scaler:
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.cfg.train.grad_clip
                )
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.cfg.train.grad_clip
                )
                self.optimizer.step()
            
            total_loss += loss.item()
            total_iou += loss_dict['iou'].item()
            n_batches += 1
            
            pbar.set_postfix({
                'L': f"{loss.item():.3f}",
                'IoU': f"{loss_dict['iou'].item():.3f}"
            })
        
        avg_loss = total_loss / max(n_batches, 1)
        avg_iou = total_iou / max(n_batches, 1)
        
        return avg_loss, avg_iou
    
    @torch.no_grad()
    def validate(self, loader: DataLoader) -> tuple:
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."""
        
        self.model.eval()
        
        total_loss = 0.0
        total_iou = 0.0
        n_batches = 0
        
        for batch in tqdm(loader, desc="Validation", ncols=100, leave=False):
            if batch is None:
                continue
            
            images = batch['image'].to(self.device, non_blocking=True)
            points = batch['points'].to(self.device, non_blocking=True)
            targets = batch['occupancies'].to(self.device, non_blocking=True)
            
            with torch.amp.autocast('cuda', enabled=self.use_amp):
                logits = self.model(images, points)
                loss_dict = self.criterion(logits, targets)
            
            total_loss += loss_dict['total'].item()
            total_iou += loss_dict['iou'].item()
            n_batches += 1
        
        avg_loss = total_loss / max(n_batches, 1)
        avg_iou = total_iou / max(n_batches, 1)
        
        return avg_loss, avg_iou
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader
    ) -> None:
        """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."""
        global STOP_TRAINING
        
        print("\n" + "=" * 60)
        print("TRAINING STARTED")
        print(f"Epochs: {self.start_epoch + 1} â†’ {self.cfg.train.num_epochs}")
        print(f"Batch size: {self.cfg.train.batch_size}")
        print(f"Learning rate: {self.cfg.train.learning_rate}")
        print(f"Category: {self.cfg.train.category_filter or 'all'}")
        print("=" * 60)
        
        for epoch in range(self.start_epoch, self.cfg.train.num_epochs):
            self.current_epoch = epoch
            epoch_start = datetime.now()
            
            if STOP_TRAINING:
                print(f"\n[train.py] Stopping before epoch {epoch + 1}")
                self.save_checkpoint(epoch - 1, reason="Stopped by user")
                break
            
            # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
            train_loss, train_iou = self.train_epoch(train_loader, epoch)
            
            if STOP_TRAINING:
                print(f"\n[train.py] Stopping after epoch {epoch + 1}")
                self.save_checkpoint(epoch, reason="Stopped by user")
                break
            
            # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ
            val_loss, val_iou = self.validate(val_loader)
            
            # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
            epoch_time = (datetime.now() - epoch_start).total_seconds()
            current_lr = self.optimizer.param_groups[0]['lr']
            
            print(f"\nEpoch {epoch + 1}/{self.cfg.train.num_epochs}")
            print(f"  Train - Loss: {train_loss:.4f}, IoU: {train_iou:.4f}")
            print(f"  Val   - Loss: {val_loss:.4f}, IoU: {val_iou:.4f}")
            print(f"  LR: {current_lr:.2e} | Time: {epoch_time:.1f}s")
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ²
            is_best = val_iou > self.best_iou
            if is_best:
                self.best_iou = val_iou
            
            is_periodic = (epoch + 1) % self.cfg.train.save_interval == 0
            
            self.save_checkpoint(
                epoch,
                is_best=is_best,
                is_periodic=is_periodic
            )
            
            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ scheduler
            self.scheduler.step()
            
            sys.stdout.flush()
        
        if not STOP_TRAINING:
            print("\n" + "=" * 60)
            print("TRAINING COMPLETE")
            print(f"Best Val IoU: {self.best_iou:.4f}")
            print(f"Checkpoints saved to: {self.cfg.paths.checkpoint_dir}")
            print("=" * 60)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞŸĞĞ Ğ¡Ğ•Ğ  ĞĞ Ğ“Ğ£ĞœĞ•ĞĞ¢ĞĞ’ ĞšĞĞœĞĞĞ”ĞĞĞ™ Ğ¡Ğ¢Ğ ĞĞšĞ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_args():
    """
    ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸.
    
    ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸Ğ· config.py Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ¾ĞºÑƒ.
    """
    parser = argparse.ArgumentParser(
        description='Train Occupancy Network on PIX3D dataset',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python train.py
    python train.py --batch_size 64 --num_epochs 300
    python train.py --category chair --learning_rate 1e-4
    python train.py --use_preprocessed
        """
    )
    
    # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    parser.add_argument('--batch_size', type=int, default=None,
                        help='Batch size (default: from config)')
    parser.add_argument('--num_epochs', type=int, default=None,
                        help='Number of epochs (default: from config)')
    parser.add_argument('--learning_rate', type=float, default=None,
                        help='Learning rate (default: from config)')
    parser.add_argument('--num_points', type=int, default=None,
                        help='Number of points per sample (default: from config)')
    
    # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    parser.add_argument('--latent_dim', type=int, default=None,
                        help='Latent dimension (default: from config)')
    
    # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ
    parser.add_argument('--category', type=str, default=None,
                        help='Category filter (chair, table, etc.)')
    parser.add_argument('--use_preprocessed', action='store_true',
                        help='Use preprocessed data for faster loading')
    
    # ĞŸÑƒÑ‚Ğ¸
    parser.add_argument('--data_root', type=str, default=None,
                        help='Path to PIX3D data')
    parser.add_argument('--checkpoint_dir', type=str, default=None,
                        help='Path to save checkpoints')
    
    # Ğ Ğ°Ğ·Ğ½Ğ¾Ğµ
    parser.add_argument('--save_interval', type=int, default=None,
                        help='Save checkpoint every N epochs')
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to checkpoint to resume from')
    
    return parser.parse_args()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN FUNCTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."""
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    args = parse_args()
    
    # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğµ-None)
    config_updates = {}
    
    if args.batch_size is not None:
        config_updates['batch_size'] = args.batch_size
    if args.num_epochs is not None:
        config_updates['num_epochs'] = args.num_epochs
    if args.learning_rate is not None:
        config_updates['learning_rate'] = args.learning_rate
    if args.num_points is not None:
        config_updates['num_points'] = args.num_points
    if args.latent_dim is not None:
        config_updates['latent_dim'] = args.latent_dim
    if args.category is not None:
        config_updates['category_filter'] = args.category if args.category != 'all' else None
    if args.use_preprocessed:
        config_updates['use_preprocessed'] = True
    if args.data_root is not None:
        config_updates['data_root'] = args.data_root
    if args.checkpoint_dir is not None:
        config_updates['checkpoint_dir'] = args.checkpoint_dir
    if args.save_interval is not None:
        config_updates['save_interval'] = args.save_interval
    
    # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ
    if config_updates:
        print("[train.py] ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸:")
        update_config(**config_updates)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    cfg = get_config()
    
    print("=" * 60)
    print("OCCUPANCY NETWORK TRAINING")
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    cfg.print_config()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° random seed
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    torch.manual_seed(cfg.train.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(cfg.train.seed)
        torch.backends.cudnn.benchmark = True
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n[1/3] Loading data...")
    
    # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ factory Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
    full_dataset = create_dataset(cfg, is_train=True)
    
    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° train/val
    n_total = len(full_dataset)
    n_val = int(n_total * cfg.train.val_split)
    n_train = n_total - n_val
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset,
        [n_train, n_val],
        generator=torch.Generator().manual_seed(cfg.train.seed)
    )
    
    print(f"[train.py] Train samples: {n_train}")
    print(f"[train.py] Val samples: {n_val}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DataLoader'Ğ¾Ğ²
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg.train.batch_size,
        shuffle=True,
        num_workers=cfg.train.num_workers,
        collate_fn=collate_fn,
        drop_last=True,
        pin_memory=cfg.train.pin_memory,
        persistent_workers=cfg.train.num_workers > 0
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=cfg.train.batch_size,
        shuffle=False,
        num_workers=max(cfg.train.num_workers // 2, 1),
        collate_fn=collate_fn,
        pin_memory=cfg.train.pin_memory
    )
    
    print(f"[train.py] Train batches: {len(train_loader)}")
    print(f"[train.py] Val batches: {len(val_loader)}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Trainer
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n[2/3] Creating model...")
    trainer = Trainer(cfg)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n[3/3] Checking for existing checkpoint...")
    
    if args.resume:
        # Ğ¯Ğ²Ğ½Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚
        trainer.load_checkpoint(args.resume)
    else:
        # ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾Ğ·Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· latest.pth
        checkpoint_path = os.path.join(cfg.paths.checkpoint_dir, 'latest.pth')
        trainer.load_checkpoint(checkpoint_path)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    try:
        trainer.train(train_loader, val_loader)
    except Exception as e:
        print(f"\n[train.py] Error during training: {e}")
        import traceback
        traceback.print_exc()
        
        trainer.save_checkpoint(
            trainer.current_epoch,
            reason=f"Error: {str(e)[:50]}"
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == '__main__':
    main()